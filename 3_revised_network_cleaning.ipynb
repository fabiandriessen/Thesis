{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from API import K\n",
    "import requests\n",
    "import json\n",
    "import googlemaps\n",
    "from dms2dec.dms_convert import dms2dec\n",
    "import geopy.distance"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import network and IVS data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import network\n",
    "G = pickle.load(open('data/network_digital_twin_v0.3.pickle', 'rb'))\n",
    "\n",
    "# import cleaned and restructured IVS data\n",
    "df_ivs = pickle.load( open(\"data/df_trips_per_path_hourly.p\", \"rb\" ) )\n",
    "\n",
    "df_h = pd.read_csv('data/cleaned_harbours.csv')\n",
    "# extract position for drawing purposes\n",
    "pos_dict = {}\n",
    "for node in G.nodes:\n",
    "    pos_dict[node] = (G.nodes[node]['X'],G.nodes[node]['Y'])\n",
    "\n",
    "#extract data\n",
    "df_links = nx.to_pandas_edgelist(G)\n",
    "df_nodes = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
    "\n",
    "# add degree to dataframes\n",
    "df_links['degree_source'] = df_links.source.apply(lambda x: G.degree[x])\n",
    "df_links['degree_target'] = df_links.source.apply(lambda x: G.degree[x])\n",
    "df_nodes['degree'] = G.degree\n",
    "df_nodes['degree'] = df_nodes.degree.apply(lambda x: x[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ivs.replace(0,np.nan, inplace=True)\n",
    "# for now only consider 100 most frequented origin destination pairs, may reconsider later\n",
    "df_ivs = df_ivs.head(100)\n",
    "# check dataframe\n",
    "df_ivs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Select relevant harbours and determine decimal degrees coordinates for all habours"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get unique harbours from IVS dataframe\n",
    "h_list = list(set(list(df_ivs.origin.unique())) | set(list(df_ivs.destination.unique())))\n",
    "\n",
    "# get unique city codes (e.g. remove NL prefix)\n",
    "h_list = [re.sub(\"NL\", \"\", i) for i in h_list]\n",
    "\n",
    "#subset harbour data for these harbours and reset index\n",
    "df_h = df_h.loc[(df_h.city_abbr.isin(h_list)) & (df_h.country == 'NL')]\n",
    "df_h.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(h_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fix dtypes\n",
    "for i in df_h.columns:\n",
    "    if df_h.dtypes[i] == 'O':\n",
    "        df_h[i] = df_h[i].astype('|S80')\n",
    "        df_h[i] = df_h[i].apply(lambda x: x.decode('utf-8'))\n",
    "# the coordinates are inaccurate and quite a lot of data is missing. This must be fixed first before we continue."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #convert latitude and longitude to decimal degrees for harbours with this entry\n",
    "df_h['lat'] = 0\n",
    "df_h['lon'] = 0\n",
    "for key, coords in enumerate(df_h.coords):\n",
    "    lat_lon=[]\n",
    "    if coords!='nan':\n",
    "        for j in range(2):\n",
    "            if j == 0:\n",
    "                part_a = coords.split()[j][:2]\n",
    "                part_b = coords.split()[j][-3:-1]\n",
    "                cor = (str(part_a)+\"°\"+str(part_b)+''''0\"N\"''')\n",
    "                cor = dms2dec(cor)\n",
    "                df_h.lat[key] = cor\n",
    "            else:\n",
    "                part_a = coords.split()[j][:3]\n",
    "                part_b = coords.split()[j][-3:-1]\n",
    "                cor = (str(part_a)+\"°\"+str(part_b)+''''0\"E\"''')\n",
    "                cor = dms2dec(cor)\n",
    "                df_h.lon[key] = cor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# still missing quite some, try and fetch these using google maps api"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try to retrieve missing data using google maps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, city in enumerate(df_h.city_full):\n",
    "    if df_h.coords[i] == 'nan':\n",
    "        r = requests.get(f\"https://maps.googleapis.com/maps/api/geocode/json?address={'Haven', city, 'Nederland'}&key={K}\")\n",
    "        results = json.loads(r.content)\n",
    "        if 'results' in results.keys():\n",
    "            if len(results['results'])>0:\n",
    "                lat_r = results['results'][0]['geometry']['location']['lat']\n",
    "                lon_r = results['results'][0]['geometry']['location']['lng']\n",
    "                df_h['lat'][i] = lat_r\n",
    "                df_h['lon'][i] = lon_r\n",
    "        else:\n",
    "            print('No location found for harbour', city)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_h.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Manually check and fill in last missing data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# some last manual changes\n",
    "# Stein mistake google API fetch, finds somewhere near soest somehow\n",
    "# Wageningen: location fetched at other side of the city\n",
    "# Geertruidenberg: inland harbour instead of harbour along waal selected\n",
    "# Genemuiden: inland harbour selected, might be problem because of curve in river around city\n",
    "# Terneuzen: inland harbour selected somehow\n",
    "# Farsum and Delfzijl: Delfzijl, mistake in coords, Farsum very closeby, safer to hard code\n",
    "d_cor_h = {'Stein':[50.974662, 5.756552], 'Wageningen':[51.955027, 5.648670], 'Geertruidenberg': [51.712726, 4.845269], 'Genemuiden':[52.629176, 6.053162], 'Terneuzen':[51.342704, 3.814359], 'Farsum':[53.314251, 6.930846], 'Delfzijl':[53.330089, 6.934031]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fill in manually found values\n",
    "for harbour in d_cor_h.keys():\n",
    "    df_h.lat[df_h.loc[df_h.city_full == harbour].index] = d_cor_h[harbour][0]\n",
    "    df_h.lon[df_h.loc[df_h.city_full == harbour].index] = d_cor_h[harbour][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic cleaning: apply bounding box en removing links from i to i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for node1, node2 in G.edges:\n",
    "    if node1 == node2:\n",
    "        print(\"Self loop identified node\", node1)\n",
    "        G.remove_edge(node1,node2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bounding box nl\n",
    "bb = (3.31497114423, 50.803721015, 7.09205325687, 53.5104033474)\n",
    "\n",
    "df_nodes = df_nodes.loc[(df_nodes.X.between(bb[0], bb[2])) & (df_nodes.Y.between(bb[1], bb[3]))]\n",
    "\n",
    "#visualise new subset\n",
    "\n",
    "#subset graph and make editable again\n",
    "G = G.subgraph(df_nodes.index)\n",
    "G = nx.Graph(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visually check network and harbours"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visual check\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "nx.draw_networkx_edges(G, pos_dict)\n",
    "plt.scatter(df_h.lon,df_h.lat,c='r')\n",
    "a = df_h.loc[df_h.lon==df_h.lon.min()]\n",
    "# plt.scatter(a.lon,a.lat,c='b')\n",
    "berth_nodes = df_nodes.loc[df_nodes.n.str.contains('Berth')].index\n",
    "berth_nodes = df_nodes.loc[df_nodes.index.isin(berth_nodes)]\n",
    "plt.scatter(berth_nodes.X,berth_nodes.Y, s=10, c='b')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: throw out all small nodes and only keep the largest component"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#extract data\n",
    "df_links = nx.to_pandas_edgelist(G)\n",
    "df_nodes = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check out codes to base selection on\n",
    "df_links.Code.unique()\n",
    "nodes_to_keep = list(df_links.loc[df_links.Code != '_0'].source) + list(df_links.loc[df_links.Code != '_0'].target)\n",
    "G = G.subgraph(nodes_to_keep)\n",
    "G = nx.Graph(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplots(dpi=200)\n",
    "nx.draw_networkx_edges(G, pos_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_links = nx.to_pandas_edgelist(G)\n",
    "df_nodes = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Determine harbour nodes\n",
    "A harbour node, is a node on the ongoing route along which the harbour is located. This node may be found because of its degree which is higher than 3 and because the link of which it is the source node, has the tag vaarwegvak 0 tot H-0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Check initial range: 5k seems to be okay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dev = 0.04\n",
    "a = 51.985103 + dev\n",
    "b = 5.898730 + dev"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "geopy.distance.geodesic((51.985103,5.898730),(a,b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create an additional column to put corresponding harbour node in\n",
    "df_h['harbour_node'] = 0\n",
    "# loop over all harbour entries\n",
    "for i in df_h.index:\n",
    "    x = df_h.lon[i]\n",
    "    y = df_h.lat[i]\n",
    "    dev = 0.04\n",
    "\n",
    "    #find nodes within deviation\n",
    "    #select nodes near\n",
    "    selection = list(df_nodes.loc[(df_nodes.X.between(x-dev, x+dev)) & (df_nodes.Y.between(y-dev, y+dev)) & (df_links.GeoType == 'section')&(df_links.source.str.isdigit())].index)\n",
    "\n",
    "    # in some areas there are very few nodes, therefore iteratively increase range to look for nodes until at least one is found\n",
    "    while len(selection) == 0:\n",
    "        dev+=0.1\n",
    "        selection = list(df_nodes.loc[(df_nodes.X.between(x-dev, x+dev)) & (df_nodes.Y.between(y-dev, y+dev))].index)\n",
    "\n",
    "    #select corresponding links and subset links that go to harbour, subset sections with origin is number (no object)\n",
    "    selection = df_links.loc[((df_links.source.isin(selection))|(df_links.target.isin(selection)))&(df_links.Name == 'Vaarwegvak van 0 tot 0 - H')&(df_links.GeoType == 'section')&(df_links.source.str.isdigit())]\n",
    "\n",
    "    if len(selection) != 0:\n",
    "        selection['dist'] = 0\n",
    "        # if there are items, take nearest, first we need to determine dists\n",
    "        for j in selection.index:\n",
    "            sel_source = selection.source[j]\n",
    "            sel_source = df_nodes.loc[sel_source]\n",
    "            selection.dist[j]=geopy.distance.geodesic((x,y),(sel_source.X,sel_source.Y))\n",
    "        # sort by dist and pick firstG\n",
    "        selection = selection.loc[selection.dist == selection.dist.min()].source\n",
    "        df_h['harbour_node'][i]= selection.values[0]\n",
    "    else:\n",
    "        selection = list(df_nodes.loc[(df_nodes.X.between(x-dev, x+dev)) & (df_nodes.Y.between(y-dev, y+dev))].index)\n",
    "        if len(selection) != 0:\n",
    "            selection = df_links.loc[((df_links.source.isin(selection))|(df_links.target.isin(selection)))&(df_links.GeoType == 'section')&(df_links.source.str.isdigit())]\n",
    "            selection['dist'] = 0\n",
    "            for j in selection.index:\n",
    "                sel_source = selection.source[j]\n",
    "                sel_source = df_nodes.loc[sel_source]\n",
    "                selection.dist[j]=geopy.distance.geodesic((x,y),(sel_source.X,sel_source.Y))\n",
    "            # sort by dist and pick firstG\n",
    "            selection = selection.loc[selection.dist == selection.dist.min()].source\n",
    "            df_h['harbour_node'][i]= selection.values[0]\n",
    "        else:\n",
    "            print('Check entry', df_h.city_full[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "harbour_nodes = list(df_h.harbour_node.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "harbour_nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplots(dpi=200)\n",
    "nx.draw_networkx_edges(G, pos_dict)\n",
    "nx.draw_networkx_nodes(G, pos_dict, harbour_nodes,node_size=10, node_color='red')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Split flows where necessary\n",
    "Large ships may not be able to take the same route as small ships, hence these should be observed as separate flows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "types = list(df_links.Code.unique())\n",
    "types.remove('_0')\n",
    "types = sorted(types[1:])\n",
    "print(types)\n",
    "#sorted in wrong manner, correct manually\n",
    "types = ['I', 'II', 'III', 'IV', 'V_A', 'V_B', 'VI_A', 'VI_B', 'VI_C']\n",
    "type_dict={}\n",
    "#minus 1 because last one is open class anyway\n",
    "for i in range(len(types)-1):\n",
    "    type_dict[i] = types[:(i+1)]\n",
    "type_dict[8]=types"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ivs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#To what extent to paths differ for these OD pairs for different boat types?\n",
    "routes_dests = {}\n",
    "routes_types_paths = {}\n",
    "routes_types_path_lengths = {}\n",
    "for i in df_ivs.index:\n",
    "\n",
    "    org_n = df_ivs.origin[i]\n",
    "    org = df_h.loc[df_h.harbour_code == org_n]['harbour_node'].values[0]\n",
    "    dest_n = df_ivs.destination[i]\n",
    "    dest = df_h.loc[df_h.harbour_code == dest_n]['harbour_node'].values[0]\n",
    "\n",
    "    org_route = nx.dijkstra_path(G, org, dest, weight='length_m')\n",
    "    routes_types_paths[(org_n, dest_n, 0)] = org_route\n",
    "    routes_types_path_lengths[(org_n, dest_n, 0)] = nx.dijkstra_path_length(G, org, dest, weight='length_m')\n",
    "    route_v = {0:types}\n",
    "    r=0\n",
    "    for type_index, types_exc in type_dict.items():\n",
    "\n",
    "        #copy graph\n",
    "        H = G.copy()\n",
    "\n",
    "        #determine links that are not available based on dict (e.g. of the type in type_exc list)\n",
    "        unavailable_edges = df_links.loc[df_links.Code.isin(types_exc)]\n",
    "        #now remove those edges\n",
    "        for link in unavailable_edges.index:\n",
    "            H.remove_edge(unavailable_edges.source[link], unavailable_edges.target[link])\n",
    "\n",
    "        if nx.has_path(H, org, dest):\n",
    "            route_type = nx.dijkstra_path(H, org, dest, weight='length_m')\n",
    "            if route_type != org_route:\n",
    "                #new route found, r higher now\n",
    "                r+=1\n",
    "                #if first time alt: old route is for exc\n",
    "                if r == 1:\n",
    "                    route_v[r-1] = tuple(types_exc)\n",
    "                #if not first time alt: previous route is for all as before minus ones that take new route\n",
    "                else:\n",
    "                    route_v[r-1] = tuple(set(route_v[r-1])-(set(types)-set(types_exc)))\n",
    "                #new route is for all not excluded types\n",
    "                route_v[r] = tuple(set(types)-set(types_exc))\n",
    "                #store new route\n",
    "                routes_types_paths[(org_n, dest_n, r)] = route_type\n",
    "                routes_types_path_lengths[(org_n, dest_n, r)] = nx.dijkstra_path_length(H, org, dest, weight='length_m')\n",
    "\n",
    "                #update most recent route\n",
    "                org_route = route_type\n",
    "\n",
    "                # print(\"Ships of type\", set(types)-set(types_exc), \"must take other route for path\", org_n, dest_n)\n",
    "    #add all routes to main dict to check\n",
    "    routes_dests[(org_n,dest_n)] = route_v\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes_dests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes_types_paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes_dests = pickle.load(open('data/revised_cleaning_results/users_ship_specific_routes.p', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(routes_types_paths.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create dict with cumulative rows that are related to each ship type\n",
    "df_ships = pd.read_excel('data/ship_types.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pregenerate ship sets for combinations on routes\n",
    "all_type_comb = []\n",
    "for i in routes_dests.keys():\n",
    "    for key, item in routes_dests[i].items():\n",
    "        if not item in all_type_comb:\n",
    "            all_type_comb.append(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cor_RWS_types = {}\n",
    "for type_combi in all_type_comb:\n",
    "    a = df_ships.loc[df_ships['CEMT-class'].isin(type_combi)]\n",
    "    a = list(a['RWS-class'].values)\n",
    "    cor_RWS_types[tuple(type_combi)] = a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cor_RWS_types\n",
    "# looks good, now all that's left is to create the exploded df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#worked, now we must fill a new dict with a row for each route version in this dictionary\n",
    "# a key for each row of the df\n",
    "dict_ivs_edited = {i:[] for i in df_ivs.columns}\n",
    "dict_ivs_edited['route_v'] = []\n",
    "\n",
    "for org, des in routes_dests.keys():\n",
    "    #first loc row to process\n",
    "    for key, types in routes_dests[(org,des)].items():\n",
    "        for hour in range(24):\n",
    "            if len(routes_dests[(org,des)].keys()) == 1:\n",
    "                dict_ivs_edited['route_v'].append(0)\n",
    "                #if there is only one route, just values to new df\n",
    "                for column in df_ivs.columns:\n",
    "                    to_a = df_ivs.loc[(df_ivs.origin == org)&(df_ivs.destination == des)&(df_ivs.hour==hour)].reset_index(drop=True).loc[0, column]\n",
    "                    dict_ivs_edited[column].append(to_a)\n",
    "            else:\n",
    "                #append route version\n",
    "                dict_ivs_edited['route_v'].append(key)\n",
    "                #copy total count, org and dest and hour\n",
    "                to_a = df_ivs.loc[(df_ivs.origin == org)&(df_ivs.destination == des)&(df_ivs.hour==hour)].reset_index(drop=True)\n",
    "                for column in to_a.iloc[:,:4].columns:\n",
    "                    dict_ivs_edited[column].append(to_a.loc[0,column])\n",
    "\n",
    "                # finally copy value if in dict and otherwise set value to 0 for other columns\n",
    "                # print(types)\n",
    "                columns_to_copy = cor_RWS_types[tuple(types)]\n",
    "                all_types = list(to_a.iloc[:,4:].columns)\n",
    "                for type1 in all_types:\n",
    "                    if type1 in columns_to_copy:\n",
    "                        dict_ivs_edited[type1].append(to_a.loc[0,type1])\n",
    "                    else:\n",
    "                        dict_ivs_edited[type1].append(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ivs_exploded = pd.DataFrame.from_dict(dict_ivs_edited)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ivs_exploded['trip_count']=df_ivs_exploded.iloc[:, 4:-1].sum(axis=1)\n",
    "df_ivs_exploded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes_types_paths = pickle.load(open(\"data/revised_cleaning_results/paths_ship_specific_routes.p\", \"rb\"))\n",
    "routes_types_path_lengths = pickle.load(open(\"data/revised_cleaning_results/path_lengths_ship_specific_routes.p\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: only keep dijkstra paths between nodes\n",
    "All paths necessary paths are already generated, all that's left is to find all original nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "harbour_nodes = list(df_h.harbour_node.unique())\n",
    "node_list = []\n",
    "for key, route in routes_types_paths.items():\n",
    "    node_list.append(route)\n",
    "\n",
    "expanded_node_list = [x for xs in node_list for x in xs]\n",
    "node_list = list(set(expanded_node_list))\n",
    "G = G.subgraph(node_list)\n",
    "G = nx.Graph(G)\n",
    "\n",
    "plt.subplots(dpi=200)\n",
    "nx.draw_networkx_edges(G, pos_dict)\n",
    "nx.draw_networkx_nodes(G, pos_dict, harbour_nodes, node_size=10, node_color='red')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Remove 0 entries from all datasets\n",
    "for i in df_ivs_exploded.index:\n",
    "    if df_ivs_exploded.trip_count[i] == 0:\n",
    "        try:\n",
    "            del routes_types_paths[(df_ivs_exploded.origin[i],df_ivs_exploded.destination[i],df_ivs_exploded.route_v[i])]\n",
    "            del routes_types_path_lengths[(df_ivs_exploded.origin[i],df_ivs_exploded.destination[i],df_ivs_exploded.route_v[i])]\n",
    "        except:\n",
    "            print(\"already gone\")\n",
    "        # try:\n",
    "        #     df_ivs_exploded=df_ivs_exploded.drop(labels=[i], axis=0)\n",
    "        # except:\n",
    "        #     print(\"already gone\")\n",
    "        del routes_dests[(df_ivs_exploded.origin[i],df_ivs_exploded.destination[i],df_ivs_exploded.route_v[i])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ivs_exploded.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes_dests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ivs_exploded.loc[df_ivs_exploded.hour==0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code below was used to conclude that different routes had to be considered for OD pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q: how many links for large ships (e.g. not I, II or III) are not in the graph now?\n",
    "Also plot links for small ships only in other color to visualise result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# # select nodes from org graph with not _0, I, II or III\n",
    "# K = pickle.load(open('data/network_digital_twin_v0.3.pickle', 'rb'))\n",
    "# df_links_or = nx.to_pandas_edgelist(K)\n",
    "# df_nodes_or = pd.DataFrame.from_dict(dict(K.nodes(data=True)), orient='index')\n",
    "#\n",
    "# # bounding box nl\n",
    "# bb = (3.31497114423, 50.803721015, 7.09205325687, 53.5104033474)\n",
    "# df_nodes_or = df_nodes_or.loc[(df_nodes_or.X.between(bb[0], bb[2])) & (df_nodes_or.Y.between(bb[1], bb[3]))]\n",
    "#\n",
    "# #subset graph and make editable again\n",
    "# K = K.subgraph(df_nodes_or.index)\n",
    "# K = nx.Graph(K)\n",
    "#\n",
    "# df_links_or = nx.to_pandas_edgelist(K)\n",
    "# df_nodes_or = pd.DataFrame.from_dict(dict(K.nodes(data=True)), orient='index')\n",
    "#\n",
    "# nx.draw_networkx_edges(K, pos_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a = df_links_or.loc[~df_links_or.Code.isin(['_0','I','II','III'])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# large_edges_full = []\n",
    "# for i in a.index:\n",
    "#     large_edges_full.append(tuple([a.source[i], a.target[i]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# len(large_edges_full)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# large_edges = []\n",
    "# large_links = df_links.loc[~df_links.Code.isin(['_0','I','II','III'])]\n",
    "# for i in large_links.index:\n",
    "#     large_edges.append(tuple([large_links.source[i], large_links.target[i]]))\n",
    "#\n",
    "# small_edges = []\n",
    "# small_links = df_links.loc[df_links.Code.isin(['_0','I','II','III'])]\n",
    "# for i in small_links.index:\n",
    "#     small_edges.append(tuple([small_links.source[i], small_links.target[i]]))\n",
    "#\n",
    "# for node1, node2 in K.edges:\n",
    "#     if node1 == node2:\n",
    "#         print(\"Self loop identified node\", node1)\n",
    "#         K.remove_edge(node1,node2)\n",
    "#\n",
    "# df_links_or = nx.to_pandas_edgelist(K)\n",
    "# df_nodes_or = pd.DataFrame.from_dict(dict(K.nodes(data=True)), orient='index')\n",
    "# large_not_in_G = list(set(large_edges_full)-set(large_edges))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# len(small_edges)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# len(large_edges)\n",
    "# # good, counts up till"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(dpi=200)\n",
    "# K = K.subgraph(large_not_in_G)\n",
    "# K = nx.Graph(K)\n",
    "#\n",
    "# nx.draw_networkx_edges(G, pos_dict,large_edges, edge_color='g',ax=ax, label='large edges subset')\n",
    "# nx.draw_networkx_edges(G, pos_dict, small_edges, edge_color='b',ax=ax, label='small edges subset')\n",
    "# nx.draw_networkx_edges(G, pos_dict, large_not_in_G, edge_color='r',ax=ax, label='large not in subset')\n",
    "# nx.draw_networkx_nodes(G, pos_dict, harbour_nodes, node_size=10, node_color='y', label='harbours')\n",
    "# plt.legend(fontsize=8)\n",
    "# plt.show()\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Intermediate checks\n",
    "1. Are all paths available of df_ivs?\n",
    "2. Which paths are unavailable and is this supposed so?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for i in df_ivs.index:\n",
    "#     org = df_ivs.origin[i]\n",
    "#     org = df_h.loc[df_h.harbour_code == org]['harbour_node'].values[0]\n",
    "#     dest = df_ivs.destination[i]\n",
    "#     dest = df_h.loc[df_h.harbour_code == dest]['harbour_node'].values[0]\n",
    "#     try:\n",
    "#         nx.dijkstra_path(G, org, dest, weight='length_m')\n",
    "#     except:\n",
    "#         print(\"Path between\", dest, \"and\", org, \"not feasible\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks okay, all path of df_ivs are available"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# types = list(df_links.Code.unique())\n",
    "# types.remove('_0')\n",
    "# types = sorted(types[1:])\n",
    "# print(types)\n",
    "# type_dict={}\n",
    "# #minus 1 because last one is open class anyway\n",
    "# for i in range(len(types)-1):\n",
    "#     type_dict[i] = types[:(i+1)]\n",
    "# type_dict[8]=types"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# type_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # code below was used to conclude that it was necessary to consider different routes for different ships.\n",
    "# # does not work after network cleaning though, but principle of step 3 is the same"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #To what extent to paths differ for these OD pairs for different boat types?\n",
    "# for i in df_ivs.index:\n",
    "#\n",
    "#     org_n = df_ivs.origin[i]\n",
    "#     org = df_h.loc[df_h.harbour_code == org_n]['harbour_node'].values[0]\n",
    "#     dest_n = df_ivs.destination[i]\n",
    "#     dest = df_h.loc[df_h.harbour_code == dest_n]['harbour_node'].values[0]\n",
    "#\n",
    "#     org_route = nx.dijkstra_path(G, org, dest, weight='length_m')\n",
    "#     for type_index, types_exc in type_dict.items():\n",
    "#\n",
    "#         #copy graph\n",
    "#         H = G.copy()\n",
    "#\n",
    "#         #determine links that are available based on dict (e.g. not of the type in type_exc list)\n",
    "#         unavailable_edges = df_links.loc[df_links.Code.isin(types_exc)]\n",
    "#         #now remove those edges\n",
    "#         for link in unavailable_edges.index:\n",
    "#             H.remove_edge(unavailable_edges.source[link], unavailable_edges.target[link])\n",
    "#\n",
    "#         if nx.has_path(H, org, dest):\n",
    "#             route_type = nx.dijkstra_path(H, org, dest, weight='length_m')\n",
    "#             if route_type != org_route:\n",
    "#                 print(\"Ships of type\", set(types)-set(types_exc), \"must take other route for path\", org_n, dest_n)\n",
    "#                 org_route = route_type\n",
    "#         else:\n",
    "#             print(\"Path between\", org_n, \"and\", dest_n, \"not feasible for types\", set(types)-set(types_exc))\n",
    "#             break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on these observations, step 3: flow splitting was inserted above"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}